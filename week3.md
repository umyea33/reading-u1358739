The three parameters that the authors found matter for LLM performance are the number of model parameters N, the size of the dataset D, and the amount of compute used for training C.  The authors also found that the performance doesn't depend on the architectural shape of the model that much.  Specifically the width and depth don't typically matter when it comes to performance.

The authors use overfitting when talking about the ratio between N and D.  If they scale N the same much as D then it doesn't give the best results or it is overfitted.  The authors say, "Empirical performance has a power-law relationship with each individual factor when not bottlenecked by the other two."  This means that the performance grows when each individual factor grows as long as there isn't one of the factors that are staying super low and killing the possible progress of the others.